{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 10\n",
    "### Author: Emily McAfee\n",
    "### New Topic Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load basic packages\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read Reuters ataset into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import NN packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.reuters\n",
    "num_of_words=10000\n",
    "(x_train, y_train), (x_test, y_test) = data.load_data(num_words=num_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it so we can read the dictionary\n",
    "# A dictionary mapping words to an integer index\n",
    "word_index = tf.keras.datasets.reuters.get_word_index()\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_review(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need input sequences to all have the same length (for the model to work)\n",
    "# Preprocess with keras\n",
    "# Only look at first 400 words in article\n",
    "max_review_length = 400\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen = max_review_length)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen = max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build and compile 3 difference models using Keras LTSM ideally improving model at each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 (sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct 1st model\n",
    "embedding_vecor_length = 32\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(keras.layers.LSTM(100))\n",
    "model.add(keras.layers.Dense(47, activation='sigmoid'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "scores = model.evaluate(x_test, y_test, verbose = 0)\n",
    "print(\"Accuracy (sigmoid): %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 (tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct 2nd model\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "\n",
    "model.add(keras.layers.LSTM(100))\n",
    "\n",
    "model.add(keras.layers.Dense(47, activation='tanh'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "scores = model.evaluate(x_test, y_test, verbose = 0)\n",
    "print(\"Accuracy (tanh): %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model 3 (relu)\n",
    "# Construct 2nd model\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vecor_length, input_length=max_review_length))\n",
    "\n",
    "model.add(keras.layers.LSTM(100))\n",
    "\n",
    "model.add(keras.layers.Dense(47, activation='relu'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "scores = model.evaluate(x_test, y_test, verbose = 0)\n",
    "print(\"Accuracy (relu): %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try  <font color = \"blue\"> binary </font> classification instead of multi-class to see if we can improve the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
    "word_index = reuters.get_word_index(path= \"reuters_word_index.json\")\n",
    "\n",
    "print('# of Training Samples: {}'.format(len(x_train)))\n",
    "print('# of Test Samples: {}'.format(len(x_test)))\n",
    "\n",
    "num_classes = max(y_train) + 1\n",
    "print('# of Classes: {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore indices as actual words\n",
    "index_to_word = {}\n",
    "for key, value in word_index.items():\n",
    "    index_to_word[value] = key\n",
    "print(' '.join([index_to_word[x] for x in x_train[0]]))\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are obviously words there that aren't helpful \n",
    "# so let's take care of that\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(x_train[0])\n",
    "print(len(x_train[0]))\n",
    "\n",
    "print(y_train[0])\n",
    "print(len(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.metrics_names)\n",
    "['loss', 'acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, verbose = 1, validation_split = 0.1)\n",
    "score = model.evaluate(x_test, y_test, batch_size = batch_size, verbose = 1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose = 0)\n",
    "print(\"Accuracy (binary classification): %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Describe and explain your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first three different models we used different activation functions: sigmoid, tanh, and relu). Instead of adding additional layers with different activations, I wanted to see how each activation performed independently on different models. Above, we see that relu on our news dataset performed at about 53%. This is not a good accuracy rate, but yet the best one out of our three. After looking online I see this problem being approached as a binary classification, which initially did not make sense, as there are 46 topics. However, I see that by *tokenizing* the data, we are able to treat it like a binary classification instead of a multiclass problem. To see how the well the model would perform as a binary classification, I ran one of those, too (with tokenization). We can see that the accuracy is much higher than nthe multi class classification (around 80%). Initially I did not have softmax as a layer, after adding it in after the initial model, I see it performed much better (~20% to 80%). Unsure if the original data was meant to be a multiclass or binary classification problem, as I am having issues implementing the LSTM into the binary model but I feel confident moving forward as I learn about new layers/approaches and my decision to only implement them once I am confident in their purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
